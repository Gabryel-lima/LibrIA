{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2702383,"sourceType":"datasetVersion","datasetId":1646010}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport string\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport tensorflow as tf\n\nclass CFG:\n    TRAIN_PATH = \"/kaggle/input/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train\"\n    LABELS = list(string.ascii_uppercase) + [\"del\", \"nothing\", \"space\"]\n    NUM_CLASSES = len(LABELS)\n    IMG_SIZE = 224\n    BATCH_SIZE = 64\n    EPOCHS = 60 # 27\n    LR = 1e-4\n    MOMENTUM = 0.9\n    SEED = 42\n\n    @staticmethod\n    def seed_everything():\n        random.seed(CFG.SEED)\n        os.environ[\"PYTHONHASHSEED\"] = str(CFG.SEED)\n        np.random.seed(CFG.SEED)\n        tf.random.set_seed(CFG.SEED)\n        torch.manual_seed(CFG.SEED)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(CFG.SEED)\n\nclass ASLNetVGG(nn.Module):\n    def __init__(self, feature_dim=512, freeze_vgg=True):\n        super().__init__()\n        # Backbone VGG16 pretrained\n        vgg = models.vgg16(pretrained=True)\n        # Convolutional features\n        self.vgg_feats = vgg.features\n        if freeze_vgg:\n            for p in self.vgg_feats.parameters(): \n                p.requires_grad = False\n        # Pooling for static feature (1x1)\n        self.pool1 = nn.AdaptiveAvgPool2d((1,1))\n        # Pooling for classifier branch (7x7)\n        self.pool2 = vgg.avgpool\n        # Classifier branch (penultimate layers)\n        orig_cls = list(vgg.classifier.children())[:-1]\n        self.asl_feats = nn.Sequential(*orig_cls)\n        # Projection\n        self.proj = nn.Linear(512 + 4096, feature_dim)\n        self.act  = nn.ReLU()\n        # Final head\n        self.classifier = nn.Linear(feature_dim, CFG.NUM_CLASSES)\n\n    def forward(self, x):\n        # x: [B, C, H, W]\n        feats = self.vgg_feats(x)         # [B,512,7,7]\n        # static path\n        f1 = self.pool1(feats)           # [B,512,1,1]\n        f1 = torch.flatten(f1,1)         # [B,512]\n        # classifier path\n        f2 = self.pool2(feats)           # [B,512,7,7]\n        f2 = torch.flatten(f2,1)         # [B,25088]\n        f2 = self.asl_feats(f2)          # [B,4096]\n        # concat + proj\n        f  = torch.cat([f1, f2], dim=1)  # [B,4608]\n        feat = self.act(self.proj(f))     # [B,feature_dim]\n        return self.classifier(feat)      # [B,NUM_CLASSES]\n\nclass LibrasDataset(Dataset):\n    def __init__(self, split='train', transform=None, val_ratio=0.2):\n        super().__init__()\n        self.transform = transform\n        samples = []\n        for idx, label in enumerate(CFG.LABELS):\n            label_dir = os.path.join(CFG.TRAIN_PATH, label)\n            if not os.path.isdir(label_dir): continue\n            for fname in os.listdir(label_dir):\n                if fname.lower().endswith(('.png','.jpg','.jpeg')):\n                    samples.append((os.path.join(label_dir, fname), idx))\n        random.shuffle(samples)\n        split_idx = int(len(samples) * (1 - val_ratio))\n        self.data = samples[:split_idx] if split == 'train' else samples[split_idx:]\n        print(f\"{split}: {len(self.data)} samples loaded\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path, label = self.data[idx]\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n# Training and evaluation\n\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    loss_accum = 0.0\n    correct = total = 0\n    for imgs, labels in loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        loss_accum += loss.item()\n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return loss_accum / len(loader), correct / total if total > 0 else 0.0\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, device):\n    model.eval()\n    loss_accum = 0.0\n    correct = total = 0\n    for imgs, labels in loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        loss_accum += loss.item()\n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return loss_accum / len(loader), correct / total if total > 0 else 0.0\n\n# Main script\n\ndef main():\n    CFG.seed_everything()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([\n        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    ])\n    train_ds = LibrasDataset('train', transform)\n    val_ds   = LibrasDataset('val',   transform)\n    train_dl = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=4)\n    val_dl   = DataLoader(val_ds,   batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=4)\n\n    model = ASLNetVGG(feature_dim=512).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=CFG.LR, momentum=CFG.MOMENTUM)\n\n    best_val = float('inf')\n    for epoch in range(1, CFG.EPOCHS + 1):\n        tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion, device)\n        vl_loss, vl_acc = eval_epoch(model, val_dl, criterion, device)\n        print(f\"[Epoch {epoch:02d}/{CFG.EPOCHS}] \"\n              f\"Train L: {tr_loss:.4f}, A: {tr_acc:.4%} | \"\n              f\"Val L: {vl_loss:.4f}, A: {vl_acc:.4%}\")\n        if vl_loss < best_val:\n            best_val = vl_loss\n            torch.save(model.state_dict(), \"v1_static_best.pt\")\n            print(\"ðŸ‘‰ Best static model saved\")\n\n    torch.save(model.state_dict(), \"v1_static_final.pt\")\n    print(\"ðŸ‘‰ Static final model saved\")\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}