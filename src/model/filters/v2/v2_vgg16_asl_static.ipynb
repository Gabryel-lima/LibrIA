{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d grassknoted/asl-alphabet\n",
        "!unzip asl-alphabet.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLY6fcZxcDc_",
        "outputId": "651491eb-40c4-4482-d6e2-978cc4bbbad1"
      },
      "id": "pLY6fcZxcDc_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜/root/.kaggleâ€™: File exists\n",
            "Dataset URL: https://www.kaggle.com/datasets/grassknoted/asl-alphabet\n",
            "License(s): GPL-2.0\n",
            "User cancelled operation\n",
            "Archive:  asl-alphabet.zip\n",
            "replace asl_alphabet_test/asl_alphabet_test/A_test.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ca86d8d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6ca86d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "outputId": "f8ee3ee7-ff60-4f97-fa93-04605b73e666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 69600 samples loaded\n",
            "val: 17400 samples loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01/30] Train L: 3.3490, A: 4.7141% | Val L: 3.3117, A: 11.3908%\n",
            "ðŸ‘‰ Best static model saved\n",
            "[Epoch 02/30] Train L: 3.2963, A: 9.8736% | Val L: 3.2467, A: 20.3506%\n",
            "ðŸ‘‰ Best static model saved\n",
            "[Epoch 03/30] Train L: 3.2331, A: 15.9626% | Val L: 3.1619, A: 27.8851%\n",
            "ðŸ‘‰ Best static model saved\n",
            "[Epoch 04/30] Train L: 3.1459, A: 21.3261% | Val L: 3.0486, A: 34.0345%\n",
            "ðŸ‘‰ Best static model saved\n",
            "[Epoch 05/30] Train L: 3.0403, A: 24.8966% | Val L: 2.9185, A: 37.8103%\n",
            "ðŸ‘‰ Best static model saved\n",
            "[Epoch 06/30] Train L: 2.9198, A: 27.6580% | Val L: 2.7764, A: 41.7011%\n",
            "ðŸ‘‰ Best static model saved\n",
            "[Epoch 07/30] Train L: 2.7932, A: 30.0963% | Val L: 2.6354, A: 43.4540%\n",
            "ðŸ‘‰ Best static model saved\n",
            "[Epoch 08/30] Train L: 2.6664, A: 32.5043% | Val L: 2.5065, A: 45.4885%\n",
            "ðŸ‘‰ Best static model saved\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-63c89ab6e119>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-63c89ab6e119>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mvl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvl_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         print(f\"[Epoch {epoch:02d}/{CFG.EPOCHS}] \"\n\u001b[1;32m    162\u001b[0m               \u001b[0;34mf\"Train L: {tr_loss:.4f}, A: {tr_acc:.4%} | \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-63c89ab6e119>\u001b[0m in \u001b[0;36meval_epoch\u001b[0;34m(model, loader, criterion, device)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "class CFG:\n",
        "    TRAIN_PATH = \"asl_alphabet_train/asl_alphabet_train\"\n",
        "    LABELS = list(string.ascii_uppercase) + [\"del\", \"nothing\", \"space\"]\n",
        "    NUM_CLASSES = len(LABELS)\n",
        "    IMG_SIZE = 224\n",
        "    BATCH_SIZE = 96\n",
        "    EPOCHS = 30\n",
        "    LR = 1e-4\n",
        "    MOMENTUM = 0.9\n",
        "    SEED = 42\n",
        "\n",
        "    @staticmethod\n",
        "    def seed_everything():\n",
        "        random.seed(CFG.SEED)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(CFG.SEED)\n",
        "        np.random.seed(CFG.SEED)\n",
        "        tf.random.set_seed(CFG.SEED)\n",
        "        torch.manual_seed(CFG.SEED)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(CFG.SEED)\n",
        "\n",
        "class ASLNetVGG(nn.Module):\n",
        "    def __init__(self, feature_dim=512, freeze_vgg=True):\n",
        "        super().__init__()\n",
        "        # Backbone VGG16 pretrained\n",
        "        vgg = models.vgg16(pretrained=True)\n",
        "        # Convolutional features\n",
        "        self.vgg_feats = vgg.features\n",
        "        if freeze_vgg:\n",
        "            for p in self.vgg_feats.parameters():\n",
        "                p.requires_grad = False\n",
        "        # Pooling for static feature (1x1)\n",
        "        self.pool1 = nn.AdaptiveAvgPool2d((1,1))\n",
        "        # Pooling for classifier branch (7x7)\n",
        "        self.pool2 = vgg.avgpool\n",
        "        # Classifier branch (penultimate layers)\n",
        "        orig_cls = list(vgg.classifier.children())[:-1]\n",
        "        self.asl_feats = nn.Sequential(*orig_cls)\n",
        "        # Projection\n",
        "        self.proj = nn.Linear(512 + 4096, feature_dim)\n",
        "        self.act  = nn.ReLU()\n",
        "        # Final head\n",
        "        self.classifier = nn.Linear(feature_dim, CFG.NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, H, W]\n",
        "        feats = self.vgg_feats(x)         # [B,512,7,7]\n",
        "        # static path\n",
        "        f1 = self.pool1(feats)           # [B,512,1,1]\n",
        "        f1 = torch.flatten(f1,1)         # [B,512]\n",
        "        # classifier path\n",
        "        f2 = self.pool2(feats)           # [B,512,7,7]\n",
        "        f2 = torch.flatten(f2,1)         # [B,25088]\n",
        "        f2 = self.asl_feats(f2)          # [B,4096]\n",
        "        # concat + proj\n",
        "        f  = torch.cat([f1, f2], dim=1)  # [B,4608]\n",
        "        feat = self.act(self.proj(f))     # [B,feature_dim]\n",
        "        return self.classifier(feat)      # [B,NUM_CLASSES]\n",
        "\n",
        "class LibrasDataset(Dataset):\n",
        "    def __init__(self, split='train', transform=None, val_ratio=0.2):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        samples = []\n",
        "        for idx, label in enumerate(CFG.LABELS):\n",
        "            label_dir = os.path.join(CFG.TRAIN_PATH, label)\n",
        "            if not os.path.isdir(label_dir): continue\n",
        "            for fname in os.listdir(label_dir):\n",
        "                if fname.lower().endswith(('.png','.jpg','.jpeg')):\n",
        "                    samples.append((os.path.join(label_dir, fname), idx))\n",
        "        random.shuffle(samples)\n",
        "        split_idx = int(len(samples) * (1 - val_ratio))\n",
        "        self.data = samples[:split_idx] if split == 'train' else samples[split_idx:]\n",
        "        print(f\"{split}: {len(self.data)} samples loaded\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Training and evaluation\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    loss_accum = 0.0\n",
        "    correct = total = 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_accum += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return loss_accum / len(loader), correct / total if total > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss_accum = 0.0\n",
        "    correct = total = 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss_accum += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return loss_accum / len(loader), correct / total if total > 0 else 0.0\n",
        "\n",
        "# Main script\n",
        "\n",
        "def main():\n",
        "    CFG.seed_everything()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "    train_ds = LibrasDataset('train', transform)\n",
        "    val_ds   = LibrasDataset('val',   transform)\n",
        "    train_dl = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = ASLNetVGG().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=CFG.LR, momentum=CFG.MOMENTUM)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    for epoch in range(1, CFG.EPOCHS + 1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion, device)\n",
        "        vl_loss, vl_acc = eval_epoch(model, val_dl, criterion, device)\n",
        "        print(f\"[Epoch {epoch:02d}/{CFG.EPOCHS}] \"\n",
        "              f\"Train L: {tr_loss:.4f}, A: {tr_acc:.4%} | \"\n",
        "              f\"Val L: {vl_loss:.4f}, A: {vl_acc:.4%}\")\n",
        "        if vl_loss < best_val:\n",
        "            best_val = vl_loss\n",
        "            torch.save(model.state_dict(), \"v2_static_best.pt\")\n",
        "            print(\"ðŸ‘‰ Best static model saved\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"v2_static_final.pt\")\n",
        "    print(\"ðŸ‘‰ Static final model saved\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}