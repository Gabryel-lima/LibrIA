{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2702383,"sourceType":"datasetVersion","datasetId":1646010}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":2562.506399,"end_time":"2025-04-26T22:55:56.655188","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-26T22:13:14.148789","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport string\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Configuration ---\nclass CFG:\n    TRAIN_PATH = \"/kaggle/input/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train\"\n    LABELS     = list(string.ascii_uppercase) + [\"del\", \"nothing\", \"space\"]\n    NUM_CLASSES= len(LABELS)\n    IMG_SIZE   = 224\n    BATCH_SIZE = 96 # 64\n    EPOCHS     = 60 # 25\n    LR         = 1e-4\n    WEIGHT_DECAY = 5e-4 # 1e-4\n    SEED       = 42\n    PATIENCE   = 7  # 5 early stopping\n\n    @staticmethod\n    def seed_everything():\n        random.seed(CFG.SEED)\n        os.environ[\"PYTHONHASHSEED\"] = str(CFG.SEED)\n        np.random.seed(CFG.SEED)\n        torch.manual_seed(CFG.SEED)\n        torch.cuda.manual_seed_all(CFG.SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# --- Dataset ---\nclass LibrasDataset(Dataset):\n    def __init__(self, split='train', transform=None, val_ratio=0.2):\n        super().__init__()\n        self.transform = transform\n        samples = []\n        for idx, label in enumerate(CFG.LABELS):\n            label_dir = os.path.join(CFG.TRAIN_PATH, label)\n            if not os.path.isdir(label_dir):\n                continue\n            for fname in os.listdir(label_dir):\n                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    samples.append((os.path.join(label_dir, fname), idx))\n        random.shuffle(samples)\n        split_idx = int(len(samples) * (1 - val_ratio))\n        self.data = samples[:split_idx] if split == 'train' else samples[split_idx:]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path, label = self.data[idx]\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n# --- Model Definition ---\nclass ASLNetVGG(nn.Module):\n    def __init__(self, feature_dim=512, freeze_vgg=True):\n        super().__init__()\n        vgg = models.vgg16(pretrained=True)\n        for m in vgg.features.modules():\n            if isinstance(m, nn.ReLU):\n                m.inplace = False\n        self.vgg_feats = vgg.features\n        if freeze_vgg:\n            for p in self.vgg_feats.parameters():\n                p.requires_grad = False\n        self.pool1 = nn.AdaptiveAvgPool2d((1,1))\n        self.pool2 = vgg.avgpool\n        orig_cls = list(vgg.classifier.children())[:-1]\n        self.asl_feats = nn.Sequential(*orig_cls)\n        self.proj = nn.Linear(512 + 4096, feature_dim)\n        self.act  = nn.ReLU()\n        self.classifier = nn.Linear(feature_dim, CFG.NUM_CLASSES)\n\n    def forward(self, x):\n        feats = self.vgg_feats(x)\n        f1 = self.pool1(feats)\n        f1 = torch.flatten(f1,1)\n        f2 = self.pool2(feats)\n        f2 = torch.flatten(f2,1)\n        f2 = self.asl_feats(f2)\n        f = torch.cat([f1, f2], dim=1)\n        feat = self.act(self.proj(f))\n        return self.classifier(feat)\n\n    @property\n    def conv5_3(self):\n        return self.vgg_feats[28]\n\n# --- Utilities ---\ndef plot_confusion_matrix(preds, labels, plot_dir):\n    cm = confusion_matrix(labels, preds)\n    plt.figure(figsize=(15,12))\n    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n                xticklabels=CFG.LABELS, yticklabels=CFG.LABELS)\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.savefig(plot_dir / \"confusion_matrix.png\")\n    plt.close()\n\n# --- Training/Evaluation Loops ---\ndef train_epoch(model, loader, optimizer, criterion, device, scaler=None):\n    model.train()\n    loss_accum, correct, total = 0.0, 0, 0\n    for imgs, labels in loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n        if scaler:\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        loss_accum += loss.item()\n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return loss_accum / len(loader), correct / total\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, device):\n    model.eval()\n    loss_accum, correct, total = 0.0, 0, 0\n    all_preds, all_labels = [], []\n    for imgs, labels in loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        loss_accum += loss.item()\n        preds = logits.argmax(dim=1)\n        all_preds.append(preds.cpu().numpy())\n        all_labels.append(labels.cpu().numpy())\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    all_preds = np.concatenate(all_preds)\n    all_labels = np.concatenate(all_labels)\n    return loss_accum / len(loader), correct / total, all_preds, all_labels\n\n# --- Main Script ---\ndef main():\n    CFG.seed_everything()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    transform_train = transforms.Compose([\n        transforms.RandomResizedCrop(CFG.IMG_SIZE, scale=(0.8,1.0)),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(0.2,0.2,0.2),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n    ])\n    transform_val = transforms.Compose([\n        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n    ])\n\n    train_ds = LibrasDataset('train', transform_train)\n    val_ds   = LibrasDataset('val',   transform_val)\n\n    # sampler for balanced classes\n    counts = np.bincount([label for _, label in train_ds.data])\n    weights = 1.0 / counts\n    sample_weights = [weights[label] for _, label in train_ds.data]\n    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n\n    train_dl = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE,\n                          sampler=sampler, num_workers=4)\n    val_dl   = DataLoader(val_ds,   batch_size=CFG.BATCH_SIZE,\n                          shuffle=False, num_workers=4)\n\n    model = ASLNetVGG(feature_dim=512, freeze_vgg=True).to(device)\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    optimizer = optim.AdamW(model.parameters(), lr=CFG.LR,\n                            weight_decay=CFG.WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n    scaler = torch.amp.GradScaler()\n\n    plot_dir = Path(\"./plots\")\n    plot_dir.mkdir(exist_ok=True)\n\n    best_val = float('inf')\n    epochs_no_improve = 0\n\n    for epoch in range(1, CFG.EPOCHS + 1):\n        tr_loss, tr_acc = train_epoch(model, train_dl, optimizer,\n                                      criterion, device, scaler)\n        vl_loss, vl_acc, vl_preds, vl_labels = eval_epoch(\n            model, val_dl, criterion, device)\n\n        print(f\"[Epoch {epoch:02d}/{CFG.EPOCHS}] \"\n              f\"Train L: {tr_loss:.4f}, A: {tr_acc:.4%} | \"\n              f\"Val L: {vl_loss:.4f}, A: {vl_acc:.4%}\")\n\n        # conf matrix\n        plot_confusion_matrix(vl_preds, vl_labels, plot_dir)\n\n        # scheduler step\n        scheduler.step(vl_loss)\n\n        # early stopping & best model\n        if vl_loss < best_val:\n            best_val = vl_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), \"filter_static_best.pt\")\n            print(\"ðŸ‘‰ Best static model saved\")\n            # progressive unfreeze\n            if epoch == 5:\n                for p in model.vgg_feats[24:].parameters():\n                    p.requires_grad = True\n                print(\"ðŸš€ Unfroze last VGG blocks for fine-tuning\")\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= CFG.PATIENCE:\n                print(f\"âœ‹ Early stopping at epoch {epoch}. No improvement in {CFG.PATIENCE} epochs.\")\n                break\n\n    torch.save(model.state_dict(), \"filter_static_final.pt\")\n    print(\"ðŸ‘‰ Static final model saved\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-30T00:46:02.722625Z","iopub.execute_input":"2025-04-30T00:46:02.722910Z"},"papermill":{"duration":2555.060463,"end_time":"2025-04-26T22:55:53.452351","exception":false,"start_time":"2025-04-26T22:13:18.391888","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}