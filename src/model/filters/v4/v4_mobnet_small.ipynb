{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-download"
   },
   "outputs": [],
   "source": [
    "# %% [1] Download Dataset\n",
    "!pip install -q kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d debashishsau/aslamerican-sign-language-aplhabet-dataset\n",
    "!unzip -q aslamerican-sign-language-aplhabet-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-definition"
   },
   "outputs": [],
   "source": [
    "# %% [2] Model Architecture (MobileNetV3 + Temporal)\n",
    "class ASLNetMobile(nn.Module):\n",
    "    def __init__(self, num_classes=29, temporal_window=5):\n",
    "        super().__init__()\n",
    "        self.temporal_window = temporal_window\n",
    "        \n",
    "        # Backbone MobileNetV3\n",
    "        self.backbone = mobilenet_v3_small(pretrained=True)\n",
    "        self.backbone.classifier = nn.Identity()  # Remove classification head\n",
    "        \n",
    "        # Landmarks branch\n",
    "        self.landmarks_fc = nn.Sequential(\n",
    "            nn.Linear(63, 64),  # 21 landmarks * 3 (x,y,z)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(576 + 64, num_classes)  # 576 (MobileNet) + 64 (landmarks)\n",
    "        \n",
    "    def forward(self, x, landmarks, history=None):\n",
    "        # Feature extraction\n",
    "        img_feat = self.backbone(x)\n",
    "        ldmk_feat = self.landmarks_fc(landmarks)\n",
    "        combined = torch.cat([img_feat, ldmk_feat], dim=1)\n",
    "        \n",
    "        # Temporal fusion\n",
    "        if history is not None:\n",
    "            combined = torch.stack(list(history) + [combined], dim=1)\n",
    "            combined = torch.mean(combined, dim=1)\n",
    "        \n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-and-data"
   },
   "outputs": [],
   "source": [
    "# %% [3] Config & Dataset\n",
    "class CFG:\n",
    "    TRAIN_PATH = \"ASL_Alphabet_Dataset/asl_alphabet_train\"\n",
    "    LABELS = list(string.ascii_uppercase) + [\"del\", \"nothing\", \"space\"]\n",
    "    NUM_CLASSES = len(LABELS)\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 30\n",
    "    LR = 3e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    DROPOUT = 0.3\n",
    "    TEMPORAL_WINDOW = 5\n",
    "    \n",
    "    # Augmentations\n",
    "    TRAIN_TRANSFORM = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    VAL_TRANSFORM = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, split='train', transform=None, val_ratio=0.2):\n",
    "        self.transform = CFG.TRAIN_TRANSFORM if split == 'train' else CFG.VAL_TRANSFORM\n",
    "        samples = []\n",
    "        for label_idx, label in enumerate(CFG.LABELS):\n",
    "            label_dir = os.path.join(CFG.TRAIN_PATH, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(label_dir):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    samples.append((os.path.join(label_dir, fname), label_idx))\n",
    "        random.shuffle(samples)\n",
    "        split_idx = int(len(samples) * (1 - val_ratio))\n",
    "        self.data = samples[:split_idx] if split == 'train' else samples[split_idx:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks_features(results):\n",
    "    \"\"\"\n",
    "    Extrai características dos landmarks de mãos detectados pelo MediaPipe.\n",
    "    Retorna um tensor com as coordenadas normalizadas.\n",
    "    \"\"\"\n",
    "    # Inicializa um vetor de características vazio\n",
    "    landmark_features = []\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        # Pega o primeiro conjunto de landmarks (primeira mão detectada)\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Extrai coordenadas x, y, z de cada ponto\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            landmark_features.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "        # Normaliza as features (opcional mas recomendado)\n",
    "        if landmark_features:\n",
    "            landmark_features = np.array(landmark_features)\n",
    "            min_val = landmark_features.min()\n",
    "            max_val = landmark_features.max()\n",
    "            landmark_features = (landmark_features - min_val) / (max_val - min_val + 1e-8)\n",
    "            landmark_features = landmark_features.tolist()\n",
    "\n",
    "    # Se não detectou landmarks, retorna vetor de zeros\n",
    "    if not landmark_features:\n",
    "        # MediaPipe Hands tem 21 pontos com x,y,z (63 features)\n",
    "        landmark_features = [0.0] * CFG.LANDMARKS_DIM\n",
    "\n",
    "    return torch.tensor(landmark_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-loop"
   },
   "outputs": [],
   "source": [
    "# %% [4] Training Loop with Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs, torch.zeros(imgs.size(0), 63).to(device))  # Dummy landmarks\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs, torch.zeros(imgs.size(0), 63).to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'accuracy': (np.array(all_preds) == np.array(all_labels)).mean(),\n",
    "        'precision': precision_score(all_labels, all_preds, average='macro'),\n",
    "        'recall': recall_score(all_labels, all_preds, average='macro'),\n",
    "        'f1': f1_score(all_labels, all_preds, average='macro')\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference-temporal"
   },
   "outputs": [],
   "source": [
    "# %% [5] Inference with Temporal Window\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "\n",
    "def temporal_inference(model_path, source=0):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ASLNetMobile().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    \n",
    "    # Temporal buffer\n",
    "    history = deque(maxlen=CFG.TEMPORAL_WINDOW)\n",
    "    \n",
    "    cap = cv2.VideoCapture(source)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process landmarks\n",
    "        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        landmarks = extract_hand_landmarks_features(results).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Preprocess image\n",
    "        img_tensor = CFG.VAL_TRANSFORM(Image.fromarray(img_rgb)).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor, landmarks, history=history)\n",
    "            history.append(output)\n",
    "            pred = output.argmax().item()\n",
    "        \n",
    "        # Display\n",
    "        cv2.putText(frame, f\"Pred: {CFG.LABELS[pred]}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow('ASL Temporal', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-mobile"
   },
   "outputs": [],
   "source": [
    "# %% [6] Export for Mobile\n",
    "# Quantização\n",
    "model = ASLNetMobile()\n",
    "model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Exportar para ONNX\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "dummy_landmarks = torch.randn(1, 63)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (dummy_input, dummy_landmarks),\n",
    "    \"asl_mobilenet_temporal.onnx\",\n",
    "    input_names=[\"image\", \"landmarks\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=13\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
