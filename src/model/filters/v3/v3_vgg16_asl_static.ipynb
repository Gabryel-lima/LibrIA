{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "pLY6fcZxcDc_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLY6fcZxcDc_",
        "outputId": "94561531-905c-4abd-a259-fd9b5a098707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Dataset URL: https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d debashishsau/aslamerican-sign-language-aplhabet-dataset\n",
        "!unzip -q aslamerican-sign-language-aplhabet-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapipe\n",
        "!pip install -q torch torchaudio\n",
        "!pip install -q tensorflow"
      ],
      "metadata": {
        "id": "BdHSiv5lUvre"
      },
      "id": "BdHSiv5lUvre",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_3xenwymWALL",
        "outputId": "aefa8d3b-5f84-4095-f9e4-9e5d00398b61"
      },
      "id": "_3xenwymWALL",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.5)\n",
            "Collecting torch==2.7.0 (from torchvision)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch==2.7.0->torchvision)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->torchvision)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->torchvision)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
            "Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy",
                  "torch",
                  "torchgen"
                ]
              },
              "id": "4d5c78f8e4fc4a76bbfb5e50438cb527"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ca86d8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ca86d8d",
        "outputId": "cc001d3f-1def-4e7a-82a8-88b75ea6ef16",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 178459 samples loaded\n",
            "val: 44615 samples loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:09<00:00, 60.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01/25] Train L: 3.2565, A: 12.4348% | Val L: 3.0632, A: 37.6152%\n",
            "👉 Best landmarks model saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 02/25] Train L: 2.9045, A: 29.0246% | Val L: 2.5820, A: 50.9604%\n",
            "👉 Best landmarks model saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 03/25] Train L: 2.4623, A: 39.8887% | Val L: 2.1213, A: 58.2652%\n",
            "👉 Best landmarks model saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 04/25] Train L: 2.0836, A: 47.2366% | Val L: 1.7635, A: 63.0640%\n",
            "👉 Best landmarks model saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "import threading\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "class CFG:\n",
        "    TRAIN_PATH = \"ASL_Alphabet_Dataset/asl_alphabet_train\"\n",
        "    LABELS = list(string.ascii_uppercase) + [\"del\", \"nothing\", \"space\"]\n",
        "    NUM_CLASSES = len(LABELS)\n",
        "    IMG_SIZE = 224\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 25\n",
        "    LR = 1e-4\n",
        "    MOMENTUM = 0.9\n",
        "    SEED = 42\n",
        "    LANDMARKS_DIM = 21 * 3  # 21 pontos com x, y, z cada\n",
        "\n",
        "    @staticmethod\n",
        "    def seed_everything():\n",
        "        random.seed(CFG.SEED)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(CFG.SEED)\n",
        "        np.random.seed(CFG.SEED)\n",
        "        tf.random.set_seed(CFG.SEED)\n",
        "        torch.manual_seed(CFG.SEED)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(CFG.SEED)\n",
        "\n",
        "def _plot_confusion_matrix(preds, labels, plot_dir):\n",
        "    \"\"\"Plot confusion matrix at end of training\"\"\"\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
        "                xticklabels=CFG.LABELS,\n",
        "                yticklabels=CFG.LABELS)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.savefig(plot_dir/\"confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "def apply_gradcam_torch(model, input_tensor, landmarks_tensor=None, target_class=None, target_layer=None):\n",
        "    \"\"\"\n",
        "    Gera mapa de calor Grad-CAM para um modelo PyTorch.\n",
        "    Args:\n",
        "        model: instância do modelo PyTorch.\n",
        "        input_tensor: Tensor[1,C,H,W] pré-processado.\n",
        "        landmarks_tensor: Tensor de landmarks (opcional).\n",
        "        target_class: índice da classe-alvo; se None, usa predição.\n",
        "        target_layer: camada convolucional alvo (nn.Module) do modelo.\n",
        "    Retorna:\n",
        "        cam: np.ndarray 2D normalizado.\n",
        "    \"\"\"\n",
        "    # garante gradientes no input\n",
        "    input_tensor = input_tensor.clone().detach().to(next(model.parameters()).device)\n",
        "    input_tensor.requires_grad_(True)\n",
        "\n",
        "    if landmarks_tensor is not None:\n",
        "        landmarks_tensor = landmarks_tensor.clone().detach().to(next(model.parameters()).device)\n",
        "        landmarks_tensor.requires_grad_(True)\n",
        "\n",
        "    activations = []\n",
        "    gradients   = []\n",
        "\n",
        "    def forward_hook(module, inp, out):\n",
        "        # clona e remove do grafo para não ficar uma view\n",
        "        activations.append(out.clone().detach())\n",
        "    def backward_hook(module, grad_in, grad_out):\n",
        "        # clona e remove do grafo para não ficar uma view\n",
        "        gradients.append(grad_out[0].clone().detach())\n",
        "\n",
        "    # registra hooks na camada alvo\n",
        "    if target_layer is None:\n",
        "        raise ValueError(\"target_layer must be provided for Grad-CAM\")\n",
        "    handle_fwd = target_layer.register_forward_hook(forward_hook)\n",
        "    # use full backward hook para versões novas do PyTorch\n",
        "    try:\n",
        "        handle_bwd = target_layer.register_full_backward_hook(backward_hook)\n",
        "    except AttributeError:\n",
        "        handle_bwd = target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    # forward\n",
        "    model.zero_grad()\n",
        "    output = model(input_tensor, landmarks_tensor) if landmarks_tensor is not None else model(input_tensor)\n",
        "    if target_class is None:\n",
        "        target_class = output.softmax(dim=1).argmax(dim=1).item()\n",
        "    # perda escalar para classe alvo\n",
        "    loss = output[0, target_class]\n",
        "    loss.backward()\n",
        "\n",
        "    # obtém ativação e gradiente\n",
        "    act = activations[0].detach()    # [1, C, h, w]\n",
        "    grad= gradients[0].detach()      # [1, C, h, w]\n",
        "\n",
        "    # peso por canal (global average pooling)\n",
        "    weights = grad.mean(dim=[2, 3], keepdim=True)  # [1, C, 1, 1]\n",
        "    cam = (weights * act).sum(dim=1).squeeze()     # [h, w]\n",
        "    cam = torch.relu(cam)\n",
        "    # normalização\n",
        "    cam -= cam.min()\n",
        "    cam /= (cam.max() + 1e-8)\n",
        "    cam = cam.cpu().numpy()\n",
        "\n",
        "    # remove hooks\n",
        "    handle_fwd.remove()\n",
        "    handle_bwd.remove()\n",
        "\n",
        "    return cam\n",
        "\n",
        "class ASLNetVGG(nn.Module):\n",
        "    def __init__(self, feature_dim=512, landmarks_dim=CFG.LANDMARKS_DIM, freeze_vgg=True):\n",
        "        super().__init__()\n",
        "        # Backbone VGG16 pretrained\n",
        "        vgg = models.vgg16(pretrained=True)\n",
        "        # 1) DESATIVA todo ReLU(inplace=True) → ReLU(inplace=False)\n",
        "        for m in vgg.features.modules():\n",
        "            if isinstance(m, nn.ReLU):\n",
        "                m.inplace = False\n",
        "        # Convolutional features\n",
        "        self.vgg_feats = vgg.features\n",
        "        if freeze_vgg:\n",
        "            for p in self.vgg_feats.parameters():\n",
        "                p.requires_grad = False\n",
        "        # Pooling for static feature (1x1)\n",
        "        self.pool1 = nn.AdaptiveAvgPool2d((1,1))\n",
        "        # Pooling for classifier branch (7x7)\n",
        "        self.pool2 = vgg.avgpool\n",
        "        # Classifier branch (penultimate layers)\n",
        "        orig_cls = list(vgg.classifier.children())[:-1]\n",
        "        self.asl_feats = nn.Sequential(*orig_cls)\n",
        "\n",
        "        # Nova branch para landmarks\n",
        "        self.landmarks_fc = nn.Sequential(\n",
        "            nn.Linear(landmarks_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "        # Projection (agora com landmarks)\n",
        "        self.proj = nn.Linear(512 + 4096 + 128, feature_dim)\n",
        "        self.act  = nn.ReLU()\n",
        "        # Final head\n",
        "        self.classifier = nn.Linear(feature_dim, CFG.NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x, landmarks=None):\n",
        "        # 1) extrai features de imagem\n",
        "        feats = self.vgg_feats(x)           # [B,512,7,7]\n",
        "        f1   = self.pool1(feats).flatten(1) # [B,512]\n",
        "        f2   = self.asl_feats(self.pool2(feats).flatten(1))  # [B,4096]\n",
        "\n",
        "        # 2) prepara landmarks (zeros se None)\n",
        "        if landmarks is None:\n",
        "            batch = x.size(0)\n",
        "            landmarks = torch.zeros(batch, CFG.LANDMARKS_DIM, device=x.device)\n",
        "        f3 = self.landmarks_fc(landmarks)   # [B,128]\n",
        "\n",
        "        # 3) concatena sempre os três vetores\n",
        "        f  = torch.cat([f1, f2, f3], dim=1) # [B,512+4096+128 = 4736]\n",
        "        feat = self.act(self.proj(f))       # [B, feature_dim]\n",
        "        return self.classifier(feat)        # [B, NUM_CLASSES]\n",
        "\n",
        "    @property\n",
        "    def conv5_3(self):\n",
        "        # retorna o último Conv2d sem registrá-lo no state_dict\n",
        "        return self.vgg_feats[28]\n",
        "\n",
        "def extract_hand_landmarks_features(results):\n",
        "    \"\"\"\n",
        "    Extrai características dos landmarks de mãos detectados pelo MediaPipe.\n",
        "    Retorna um tensor com as coordenadas normalizadas.\n",
        "    \"\"\"\n",
        "    # Inicializa um vetor de características vazio\n",
        "    landmark_features = []\n",
        "\n",
        "    if results.multi_hand_landmarks:\n",
        "        # Pega o primeiro conjunto de landmarks (primeira mão detectada)\n",
        "        hand_landmarks = results.multi_hand_landmarks[0]\n",
        "\n",
        "        # Extrai coordenadas x, y, z de cada ponto\n",
        "        for landmark in hand_landmarks.landmark:\n",
        "            landmark_features.extend([landmark.x, landmark.y, landmark.z])\n",
        "\n",
        "        # Normaliza as features (opcional mas recomendado)\n",
        "        if landmark_features:\n",
        "            landmark_features = np.array(landmark_features)\n",
        "            min_val = landmark_features.min()\n",
        "            max_val = landmark_features.max()\n",
        "            landmark_features = (landmark_features - min_val) / (max_val - min_val + 1e-8)\n",
        "            landmark_features = landmark_features.tolist()\n",
        "\n",
        "    # Se não detectou landmarks, retorna vetor de zeros\n",
        "    if not landmark_features:\n",
        "        # MediaPipe Hands tem 21 pontos com x,y,z (63 features)\n",
        "        landmark_features = [0.0] * CFG.LANDMARKS_DIM\n",
        "\n",
        "    return torch.tensor(landmark_features, dtype=torch.float32)\n",
        "\n",
        "class LibrasDataset(Dataset):\n",
        "    def __init__(self, split='train', transform=None, val_ratio=0.2):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        samples = []\n",
        "        for idx, label in enumerate(CFG.LABELS):\n",
        "            label_dir = os.path.join(CFG.TRAIN_PATH, label)\n",
        "            if not os.path.isdir(label_dir): continue\n",
        "            for fname in os.listdir(label_dir):\n",
        "                if fname.lower().endswith(('.png','.jpg','.jpeg')):\n",
        "                    samples.append((os.path.join(label_dir, fname), idx))\n",
        "        random.shuffle(samples)\n",
        "        split_idx = int(len(samples) * (1 - val_ratio))\n",
        "        self.data = samples[:split_idx] if split == 'train' else samples[split_idx:]\n",
        "        print(f\"{split}: {len(self.data)} samples loaded\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Training and evaluation\n",
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    loss_accum = 0.0\n",
        "    correct = total = 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_accum += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return loss_accum / len(loader), correct / total if total > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss_accum = 0.0\n",
        "    correct = total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss_accum += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Coleta predições e labels para matriz de confusão\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Concatena todos os batches\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    return (loss_accum / len(loader),\n",
        "            correct / total if total > 0 else 0.0,\n",
        "            all_preds,\n",
        "            all_labels)\n",
        "\n",
        "# Main script\n",
        "def main():\n",
        "    CFG.seed_everything()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "    train_ds = LibrasDataset('train', transform)\n",
        "    val_ds   = LibrasDataset('val',   transform)\n",
        "    train_dl = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = ASLNetVGG(feature_dim=512).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=CFG.LR, momentum=CFG.MOMENTUM)\n",
        "\n",
        "    from pathlib import Path\n",
        "    plot_dir = Path(\"./plots\")\n",
        "    plot_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    for epoch in range(1, CFG.EPOCHS + 1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion, device)\n",
        "        vl_loss, vl_acc, vl_preds, vl_labels = eval_epoch(model, val_dl, criterion, device)\n",
        "        print(f\"[Epoch {epoch:02d}/{CFG.EPOCHS}] \"\n",
        "              f\"Train L: {tr_loss:.4f}, A: {tr_acc:.4%} | \"\n",
        "              f\"Val L: {vl_loss:.4f}, A: {vl_acc:.4%}\")\n",
        "\n",
        "        # gera e salva a matriz a cada época\n",
        "        _plot_confusion_matrix(vl_preds, vl_labels, plot_dir)\n",
        "\n",
        "        if vl_loss < best_val:\n",
        "            best_val = vl_loss\n",
        "            torch.save(model.state_dict(), \"filter_static_landmarks_best.pt\")\n",
        "            print(\"👉 Best landmarks model saved\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"filter_static_landmarks_final.pt\")\n",
        "    print(\"👉 Landmarks final model saved\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Classe que faz a captura contínua em thread separada\n",
        "# --------------------------------------------------\n",
        "class VideoStream:\n",
        "    def __init__(self, source=0):\n",
        "        # abre o device de vídeo (webcam ou DroidCam)\n",
        "        self.cap = cv2.VideoCapture(source)\n",
        "        # lê o primeiro frame\n",
        "        self.grabbed, self.frame = self.cap.read()\n",
        "        # flag para encerrar a thread\n",
        "        self.stopped = False\n",
        "        # inicia a thread de captura em background\n",
        "        threading.Thread(target=self.update, daemon=True).start()\n",
        "\n",
        "    def update(self):\n",
        "        # loop de captura: sempre atualiza self.frame com o último frame disponível\n",
        "        while not self.stopped:\n",
        "            self.grabbed, self.frame = self.cap.read()\n",
        "\n",
        "    def read(self):\n",
        "        # retorna o frame mais recente lido pela thread\n",
        "        return self.grabbed, self.frame\n",
        "\n",
        "    def stop(self):\n",
        "        # sinaliza para encerrar o loop e libera o dispositivo\n",
        "        self.stopped = True\n",
        "        self.cap.release()\n",
        "\n",
        "\n",
        "def inference_with_gradcam_and_hands(model_path, source=0, target_layer_name='conv5_3', inference_fps=10):\n",
        "    # ------------------------\n",
        "    # 1) Configurações iniciais\n",
        "    # ------------------------\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = ASLNetVGG().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],\n",
        "                             [0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 2) Inicia a captura em background com VideoStream\n",
        "    # ------------------------------------------------\n",
        "    vs = VideoStream(source=source)\n",
        "    # calcula o intervalo entre quadros para manter FPS constante\n",
        "    frame_interval = 1.0 / inference_fps\n",
        "\n",
        "    # --------------------------------------------\n",
        "    # 3) Inicializa o MediaPipe Hands e o desenho\n",
        "    # --------------------------------------------\n",
        "    mp_hands = mp.solutions.hands\n",
        "    hands = mp_hands.Hands(\n",
        "        static_image_mode=False,\n",
        "        max_num_hands=2,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "    mp_draw = mp.solutions.drawing_utils\n",
        "    drawing_spec = mp_draw.DrawingSpec(\n",
        "        thickness=2, circle_radius=4, color=(0, 0, 255)\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # 4) Seleciona a camada de interesse para o Grad-CAM\n",
        "    # ------------------------------------------------\n",
        "    target_layer = getattr(model, target_layer_name, None)\n",
        "    if not isinstance(target_layer, nn.Conv2d):\n",
        "        # se não existir ou não for Conv2d, pega o último conv da lista\n",
        "        for m in reversed(model.vgg_feats):\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                target_layer = m\n",
        "                break\n",
        "    print(f\">>> {target_layer} selecionada como target layer\")\n",
        "\n",
        "    # ------------------------\n",
        "    # 5) Variáveis para estabilização de predição\n",
        "    # ------------------------\n",
        "    prediction_history = []\n",
        "    history_size = 5  # Tamanho da janela de estabilização\n",
        "    current_prediction = 0  # Predição inicial\n",
        "    min_confidence = 0.6  # Limite mínimo de confiança\n",
        "\n",
        "    # ------------------------\n",
        "    # 6) Loop principal de inferência\n",
        "    # ------------------------\n",
        "    try:\n",
        "        while True:\n",
        "            loop_start = time.time()\n",
        "\n",
        "            # a) lê o frame mais novo\n",
        "            ret, frame = vs.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # b) converte para RGB e processa mãos\n",
        "            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(img_rgb)\n",
        "\n",
        "            # c) Extrai landmarks como features\n",
        "            landmark_features = extract_hand_landmarks_features(results)\n",
        "            landmark_features = landmark_features.unsqueeze(0).to(device)  # Adiciona dim de batch\n",
        "\n",
        "            # d) desenha os landmarks detectados\n",
        "            if results.multi_hand_landmarks:\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    mp_draw.draw_landmarks(\n",
        "                        frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
        "                        drawing_spec, drawing_spec\n",
        "                    )\n",
        "\n",
        "            # e) Prepara a imagem para o modelo\n",
        "            inp = preprocess(Image.fromarray(img_rgb)).unsqueeze(0).to(device)\n",
        "\n",
        "            # f) gera o Grad-CAM com landmarks\n",
        "            cam = apply_gradcam_torch(model, inp, landmark_features, None, target_layer)\n",
        "            heatmap = cv2.resize(\n",
        "                (cam * 255).astype('uint8'),\n",
        "                (frame.shape[1], frame.shape[0])\n",
        "            )\n",
        "            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "            overlay = cv2.addWeighted(frame, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "            # g) faz a predição com landmarks e obtém o rótulo\n",
        "            with torch.no_grad():\n",
        "                output = model(inp, landmark_features)\n",
        "                confidence = torch.nn.functional.softmax(output, dim=1).max().item()\n",
        "                pred = output.softmax(1).argmax(1).item()\n",
        "\n",
        "                # Só atualiza se a confiança for alta o suficiente\n",
        "                if confidence > min_confidence:\n",
        "                    # Adiciona à história para estabilização\n",
        "                    prediction_history.append(pred)\n",
        "                    if len(prediction_history) > history_size:\n",
        "                        prediction_history.pop(0)\n",
        "\n",
        "                    # Usa a predição mais frequente\n",
        "                    most_common_pred = Counter(prediction_history).most_common(1)[0][0]\n",
        "                    current_prediction = most_common_pred\n",
        "\n",
        "            # Usa a predição estabilizada\n",
        "            label = CFG.LABELS[current_prediction]\n",
        "\n",
        "            # h) Mostra informações na tela\n",
        "            cv2.putText(\n",
        "                overlay, f\"Pred: {label} ({confidence:.2f})\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2\n",
        "            )\n",
        "\n",
        "            # i) exibe o resultado na janela\n",
        "            cv2.imshow('Hands + Grad-CAM + Landmarks', overlay)\n",
        "\n",
        "            # j) throttle para manter FPS constante\n",
        "            elapsed = time.time() - loop_start\n",
        "            if elapsed < frame_interval:\n",
        "                time.sleep(frame_interval - elapsed)\n",
        "\n",
        "            # k) condicional de saída ('q' para sair)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "    finally:\n",
        "        # ----------------------------\n",
        "        # 7) Limpeza de recursos\n",
        "        # ----------------------------\n",
        "        vs.stop()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # inference_with_gradcam_and_hands(\n",
        "    #     \"src/model/filters/v1/v1_static_91.pt\",  # Atualize para o caminho do seu modelo\n",
        "    #     source=0,  # Use 0 para webcam padrão\n",
        "    #     target_layer_name='conv5_3',\n",
        "    #     inference_fps=15  # Ajuste conforme FPS desejado e capacidade do hardware\n",
        "    # )\n",
        "    main()\n"
      ]
    },
    {
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('filter_static_landmarks_best.pt') # Baixa o modelo \"filter_static_landmarks_best.pt\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uuULLysslDV5"
      },
      "id": "uuULLysslDV5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}