{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2afd7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# 1. Instalação de dependências\n",
    "!pip install -q ultralytics tensorflow opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1eb09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# 2. Imports e configurações gerais\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Paths (ajuste conforme sua estrutura)\n",
    "DATASET_DIR = \"/kaggle/input/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train\"\n",
    "CROPS_DIR   = \"/mnt/data/hand_crops\"\n",
    "IMG_SIZE    = 224\n",
    "BATCH_SIZE  = 32\n",
    "EPOCHS      = 20\n",
    "VALID_SPLIT = 0.2\n",
    "SEED        = 42\n",
    "\n",
    "# Cria diretório para crops se não existir\n",
    "os.makedirs(CROPS_DIR, exist_ok=True)\n",
    "\n",
    "# %% [code]\n",
    "# 3. Detecção e geração dos crops (por classe)\n",
    "def create_hand_crops():\n",
    "    # Carrega o modelo YOLOv5n pré-treinado\n",
    "    model = YOLO(\"yolov5n.pt\")\n",
    "    # Itera por cada classe no dataset original\n",
    "    for label in os.listdir(DATASET_DIR):\n",
    "        src_dir = os.path.join(DATASET_DIR, label)\n",
    "        dst_dir = os.path.join(CROPS_DIR, label)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        for fname in os.listdir(src_dir):\n",
    "            if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "            img_path = os.path.join(src_dir, fname)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            results = model(img, device='cpu')\n",
    "            # Cada resultado contém múltiplas boxes\n",
    "            for i, box in enumerate(results[0].boxes.xyxy.cpu().numpy()):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "                out_path = os.path.join(dst_dir, f\"{os.path.splitext(fname)[0]}_{i}.jpg\")\n",
    "                # Converte BGR -> RGB antes de salvar\n",
    "                cv2.imwrite(out_path, cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Executa a função de geração de crops\n",
    "display(\"Gerando crops de mão... esta etapa pode demorar alguns minutos\")\n",
    "create_hand_crops()\n",
    "\n",
    "# %% [code]\n",
    "# 4. Criação dos datasets de treino e validação a partir dos crops\n",
    "dataset_train = image_dataset_from_directory(\n",
    "    CROPS_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    validation_split=VALID_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "dataset_val = image_dataset_from_directory(\n",
    "    CROPS_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    validation_split=VALID_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Obtém número de classes automaticamente\n",
    "auto_class_count = len(dataset_train.class_names)\n",
    "print(f\"Número de classes detectadas: {auto_class_count}\")\n",
    "\n",
    "# %% [code]\n",
    "# 5. Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "])\n",
    "\n",
    "# Aplica augmentation somente no treinamento\n",
    "dataset_train = dataset_train.map(lambda x, y: (data_augmentation(x), y))\n",
    "# Pré-carregamento\n",
    "dataset_train = dataset_train.prefetch(tf.data.AUTOTUNE)\n",
    "dataset_val   = dataset_val.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# %% [code]\n",
    "# 6. Definição do modelo ResNet base\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "base_model.trainable = True\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input((IMG_SIZE, IMG_SIZE, 3)),\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(auto_class_count, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "# %% [code]\n",
    "# 7. Compilação e treinamento\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=dataset_val\n",
    ")\n",
    "\n",
    "# %% [code]\n",
    "# 8. Avaliação final\n",
    "eval_loss, eval_acc = model.evaluate(dataset_val)\n",
    "print(f\"Val Loss: {eval_loss:.4f}, Val Accuracy: {eval_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
