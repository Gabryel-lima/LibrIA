{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2702383,"sourceType":"datasetVersion","datasetId":1646010}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5bed64ca-4431-4845-a666-5e944430b038","cell_type":"code","source":"# %% [markdown]\n# Pipeline Completo: Detecção de Mãos, Geração de Crops e Treinamento ResNet\nEste notebook realiza todo o fluxo em uma única execução:\n1. Instalação de dependências\n2. Detecção offline de mãos com YOLOv5n\n3. Geração e salvamento de crops por classe\n4. Criação de `tf.data.Dataset` a partir dos crops\n5. Aplicação de data augmentation\n6. Definição e compilação do modelo ResNet\n7. Treinamento e avaliação","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"9f29ee4b-c135-4a10-99d2-6d75d87de461","cell_type":"code","source":"# %% [code]\n# 1. Instalação de dependências\n!pip install ultralytics tensorflow opencv-python-headless","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"id":"d3c1eb09","cell_type":"code","source":"# %% [code]\n# 2. Imports e configurações gerais\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom ultralytics import YOLO\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Paths (ajuste conforme sua estrutura)\nDATASET_DIR = \"/mnt/data/ASL_Alphabet_Dataset/asl_alphabet_train\"\nCROPS_DIR   = \"/mnt/data/hand_crops\"\nIMG_SIZE    = 224\nBATCH_SIZE  = 32\nEPOCHS      = 20\nVALID_SPLIT = 0.2\nSEED        = 42\n\n# Cria diretório para crops se não existir\nos.makedirs(CROPS_DIR, exist_ok=True)\n\n# %% [code]\n# 3. Detecção e geração dos crops (por classe)\ndef create_hand_crops():\n    # Carrega o modelo YOLOv5n pré-treinado\n    model = YOLO(\"yolov5n.pt\")\n    # Itera por cada classe no dataset original\n    for label in os.listdir(DATASET_DIR):\n        src_dir = os.path.join(DATASET_DIR, label)\n        dst_dir = os.path.join(CROPS_DIR, label)\n        os.makedirs(dst_dir, exist_ok=True)\n        for fname in os.listdir(src_dir):\n            if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n                continue\n            img_path = os.path.join(src_dir, fname)\n            img = cv2.imread(img_path)\n            if img is None:\n                continue\n            results = model(img, device='cpu')\n            # Cada resultado contém múltiplas boxes\n            for i, box in enumerate(results[0].boxes.xyxy.cpu().numpy()):\n                x1, y1, x2, y2 = map(int, box)\n                crop = img[y1:y2, x1:x2]\n                if crop.size == 0:\n                    continue\n                out_path = os.path.join(dst_dir, f\"{os.path.splitext(fname)[0]}_{i}.jpg\")\n                # Converte BGR -> RGB antes de salvar\n                cv2.imwrite(out_path, cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n\n# Executa a função de geração de crops\ndisplay(\"Gerando crops de mão... esta etapa pode demorar alguns minutos\")\ncreate_hand_crops()\n\n# %% [code]\n# 4. Criação dos datasets de treino e validação a partir dos crops\ndataset_train = image_dataset_from_directory(\n    CROPS_DIR,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    validation_split=VALID_SPLIT,\n    subset=\"training\",\n    seed=SEED\n)\n\ndataset_val = image_dataset_from_directory(\n    CROPS_DIR,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    validation_split=VALID_SPLIT,\n    subset=\"validation\",\n    seed=SEED\n)\n\n# Obtém número de classes automaticamente\nauto_class_count = len(dataset_train.class_names)\nprint(f\"Número de classes detectadas: {auto_class_count}\")\n\n# %% [code]\n# 5. Data augmentation\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomTranslation(0.1, 0.1),\n])\n\n# Aplica augmentation somente no treinamento\ndataset_train = dataset_train.map(lambda x, y: (data_augmentation(x), y))\n# Pré-carregamento\ndataset_train = dataset_train.prefetch(tf.data.AUTOTUNE)\ndataset_val   = dataset_val.prefetch(tf.data.AUTOTUNE)\n\n# %% [code]\n# 6. Definição do modelo ResNet base\nbase_model = tf.keras.applications.ResNet50(\n    include_top=False,\n    weights=None,\n    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n)\nbase_model.trainable = True\n\nmodel = models.Sequential([\n    layers.Input((IMG_SIZE, IMG_SIZE, 3)),\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(auto_class_count, activation='softmax')\n])\nmodel.summary()\n\n# %% [code]\n# 7. Compilação e treinamento\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    dataset_train,\n    epochs=EPOCHS,\n    validation_data=dataset_val\n)\n\n# %% [code]\n# 8. Avaliação final\neval_loss, eval_acc = model.evaluate(dataset_val)\nprint(f\"Val Loss: {eval_loss:.4f}, Val Accuracy: {eval_acc:.4f}\")","metadata":{"vscode":{"languageId":"plaintext"},"editable":false},"outputs":[],"execution_count":null}]}