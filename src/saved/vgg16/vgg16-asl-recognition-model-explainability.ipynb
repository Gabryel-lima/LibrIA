{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079}],"dockerImageVersionId":30476,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style = \"color: Black; \n              display: fill;\n              border-radius: 12px;\n              background-color: #76503d;\n              box-shadow: rgba(0, 0, 0, 0.19) 0px 10px 5px, rgba(0, 0, 0, 0.15) 0px 6px 6px;\">\n    <h1 id = \"title\"\n        style = \"padding: 15px; \n                 text-align:center;\n                 color: White;\n                 font-size: 28px;\n                 font-weight: bold;\n                 font-family: Calibri;\">VGG16 ASL Recognition + Model Explainability\n        <a class=\"anchor-link\" id=\"title\" href=\"https://www.kaggle.com/code/harits/vgg16-asl-recognition-model-explainability#title\">¶</a>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"![American Sign Language History | Woodhaven](https://www.woodhaventeam.org/wp-content/uploads/2022/04/GettyImages-1182224687-scaled.jpg)","metadata":{}},{"cell_type":"markdown","source":"<div style = \"color: Black; \n              display: fill;\n              border-radius: 12px;\n              background-color: #c7a48b;\n              box-shadow: rgba(0, 0, 0, 0.15) 0px 10px 5px, rgba(0, 0, 0, 0.12) 0px 6px 6px;\">\n    <h1 id = \"preparation\"\n        style = \"padding: 13px; \n                 color: White;\n                 font-size: 24px;\n                 font-weight: bold;\n                 font-family: Calibri;\">1. | Preparation\n        <a class=\"anchor-link\" id=\"preparation\" href=\"https://www.kaggle.com/code/harits/vgg16-asl-recognition-model-explainability#preparation\">¶</a>\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"!pip install imutils","metadata":{"execution":{"iopub.status.busy":"2025-04-21T19:01:13.279754Z","execution_failed":"2025-04-21T20:08:22.275Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Libraries\n\n# Warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Main\nimport os\nimport glob\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport gc\nimport string\nimport time\nimport random\nimport imutils\nfrom PIL import Image\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom sklearn.manifold import TSNE\n\n# Model\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom keras.models import load_model, Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nclass CFG:\n    batch_size = 64\n    img_height = 224\n    img_width = 224\n    epochs = 10\n    num_classes = 29\n    img_channels = 3\n    \ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style = \"color: Black; \n              display: fill;\n              border-radius: 12px;\n              background-color: #c7a48b;\n              box-shadow: rgba(0, 0, 0, 0.15) 0px 10px 5px, rgba(0, 0, 0, 0.12) 0px 6px 6px;\">\n    <h1 id = \"dataset\"\n        style = \"padding: 13px; \n                 color: White;\n                 font-size: 24px;\n                 font-weight: bold;\n                 font-family: Calibri;\">2. | Dataset\n        <a class=\"anchor-link\" id=\"dataset\" href=\"https://www.kaggle.com/code/harits/vgg16-asl-recognition-model-explainability#dataset\">¶</a>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">2.1. Data Exploration</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Labels\nTRAIN_PATH = \"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\"\nlabels = []\nalphabet = list(string.ascii_uppercase)\nlabels.extend(alphabet)\nlabels.extend([\"del\", \"nothing\", \"space\"])\nprint(labels)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_images(labels):\n    # Create Subplots\n    y_size = 12\n    if(len(labels)<10):\n        y_size = y_size * len(labels) / 10\n    fig, axs = plt.subplots(len(labels), 9, figsize=(y_size, 13))\n\n    for i, label in enumerate(labels):\n        axs[i, 0].text(0.5, 0.5, label, ha='center', va='center', fontsize=8)\n        axs[i, 0].axis('off')\n\n        label_path = os.path.join(TRAIN_PATH, label)\n        list_files = os.listdir(label_path)\n\n        for j in range(8):\n            img_label = cv2.imread(os.path.join(label_path, list_files[j]))\n            img_label = cv2.cvtColor(img_label, cv2.COLOR_BGR2RGB)\n            axs[i, j+1].imshow(img_label)\n            axs[i, j+1].axis(\"off\")\n\n    # Title\n    plt.suptitle(\"Sample Images in ASL Alphabet Dataset\", x=0.55, y=0.92)\n\n    # Show\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_images(labels[:10])","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_images(labels[10:20])","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_images(labels[20:])","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">2.2. Data Preprocessing</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Create Metadata\nlist_path = []\nlist_labels = []\nfor label in labels:\n    label_path = os.path.join(TRAIN_PATH, label, \"*\")\n    image_files = glob.glob(label_path)\n    \n    sign_label = [label] * len(image_files)\n    \n    list_path.extend(image_files)\n    list_labels.extend(sign_label)\n\nmetadata = pd.DataFrame({\n    \"image_path\": list_path,\n    \"label\": list_labels\n})\n\nmetadata","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split Dataset to Train 0.7, Val 0.15, and Test 0.15\nX_train, X_test, y_train, y_test = train_test_split(\n    metadata[\"image_path\"], metadata[\"label\"], \n    test_size=0.15, \n    random_state=2023, \n    shuffle=True, \n    stratify=metadata[\"label\"]\n)\ndata_train = pd.DataFrame({\n    \"image_path\": X_train,\n    \"label\": y_train\n})\n\nX_train, X_val, y_train, y_val = train_test_split(\n    data_train[\"image_path\"], data_train[\"label\"],\n    test_size=0.15/0.70,\n    random_state=2023,\n    shuffle=True,\n    stratify=data_train[\"label\"]\n)\ndata_train = pd.DataFrame({\n    \"image_path\": X_train,\n    \"label\": y_train\n})\ndata_val = pd.DataFrame({\n    \"image_path\": X_val,\n    \"label\": y_val\n})\ndata_test = pd.DataFrame({\n    \"image_path\": X_test,\n    \"label\": y_test\n})\n\ndisplay(data_train)\ndisplay(data_val)\ndisplay(data_test)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Augmentation (Just Rescale)\ndef data_augmentation():\n    datagen = ImageDataGenerator(rescale=1/255.,)\n    # Training Dataset\n    train_generator = datagen.flow_from_dataframe(\n        data_train,\n        directory=\"./\",\n        x_col=\"image_path\",\n        y_col=\"label\",\n        class_mode=\"categorical\",\n        batch_size=CFG.batch_size,\n        target_size=(CFG.img_height, CFG.img_width),\n    )\n\n    # Validation Dataset\n    validation_generator = datagen.flow_from_dataframe(\n        data_val,\n        directory=\"./\",\n        x_col=\"image_path\",\n        y_col=\"label\",\n        class_mode=\"categorical\",\n        batch_size=CFG.batch_size,\n        target_size=(CFG.img_height, CFG.img_width),\n    )\n    \n    # Testing Dataset\n    test_generator = datagen.flow_from_dataframe(\n        data_test,\n        directory=\"./\",\n        x_col=\"image_path\",\n        y_col=\"label\",\n        class_mode=\"categorical\",\n        batch_size=1,\n        target_size=(CFG.img_height, CFG.img_width),\n        shuffle=False\n    )\n    \n    return train_generator, validation_generator, test_generator","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_everything(2023)\ntrain_generator, validation_generator, test_generator = data_augmentation()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.280Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style = \"color: Black; \n              display: fill;\n              border-radius: 12px;\n              background-color: #c7a48b;\n              box-shadow: rgba(0, 0, 0, 0.15) 0px 10px 5px, rgba(0, 0, 0, 0.12) 0px 6px 6px;\">\n    <h1 id = \"model-training\"\n        style = \"padding: 13px; \n                 color: White;\n                 font-size: 24px;\n                 font-weight: bold;\n                 font-family: Calibri;\">3. | Model Training\n        <a class=\"anchor-link\" id=\"model-training\" href=\"https://www.kaggle.com/code/harits/vgg16-asl-recognition-model-explainability#model-training\">¶</a>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">3.1. VGG16</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Load VGG16 model and modify for ASL recognition\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(CFG.img_height, CFG.img_width, CFG.img_channels))\n\nfor layer in base_model.layers:\n    if \"block5\" in layer.name:\n        layer.trainable = True\n\nx = base_model.output\nx = Flatten()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(29, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\ndisplay(model.summary())\ndisplay(tf.keras.utils.plot_model(model, to_file='vgg16.png', show_shapes=True))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">3.2. Training</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Compile and train the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint = ModelCheckpoint('asl_vgg16_best_weights.h5', save_best_only=True, monitor='val_accuracy', mode='max')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the Model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // CFG.batch_size,\n    epochs=CFG.epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // CFG.batch_size,\n    callbacks=[checkpoint]\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style = \"color: Black; \n              display: fill;\n              border-radius: 12px;\n              background-color: #c7a48b;\n              box-shadow: rgba(0, 0, 0, 0.15) 0px 10px 5px, rgba(0, 0, 0, 0.12) 0px 6px 6px;\">\n    <h1 id = \"model-evaluation\"\n        style = \"padding: 13px; \n                 color: White;\n                 font-size: 24px;\n                 font-weight: bold;\n                 font-family: Calibri;\">4. | Model Evaluation\n        <a class=\"anchor-link\" id=\"model-evaluation\" href=\"https://www.kaggle.com/code/harits/vgg16-asl-recognition-model-explainability#model-evaluation\">¶</a>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">4.1. Model Testing</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"scores = model.evaluate(test_generator)\nprint(\"%s: %.2f%%\" % (\"Evaluate Test Accuracy\", scores[1]*100))\n\n# Após o treinamento\nmodel.save(\"/kaggle/working/asl_model.keras\", save_format=\"keras_v3\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">4.2. Training Loss and Metrics</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Visualize Training and Validation Results\n\n# Create Subplot\nfig = make_subplots(\n        rows=1, cols=2,\n        subplot_titles=[\"Model Loss\", \"Model Accuracy\"], \n)\n\n# Configuration Plot\nclass PlotCFG:\n    marker_size = 8\n    line_size = 2\n    train_color = \"#76503d\"\n    valid_color = \"#deb392\"\n\n# Loss Plot\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(1, len(loss)+1), y=loss,\n        mode=\"markers+lines\",\n        marker=dict(\n            color=PlotCFG.train_color, size=PlotCFG.marker_size,\n            line=dict(color=\"White\", width=0.5)\n        ),\n        line=dict(color=PlotCFG.train_color, width=PlotCFG.line_size),\n        name=\"Training Loss\"\n    ), row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(1, len(val_loss)+1), y=val_loss,\n        mode=\"markers+lines\",\n        marker=dict(\n            color=PlotCFG.valid_color, size=PlotCFG.marker_size,\n            line=dict(color=\"White\", width=0.5)\n        ),\n        line=dict(color=PlotCFG.valid_color, width=PlotCFG.line_size),\n        name=\"Validation Loss\"\n    ), row=1, col=1\n)\n\n# Accuracy Plot\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(1, len(acc)+1), y=acc,\n        mode=\"markers+lines\",\n        marker=dict(\n            color=PlotCFG.train_color, size=PlotCFG.marker_size,\n            line=dict(color=\"White\", width=0.5)\n        ),\n        line=dict(color=PlotCFG.train_color, width=PlotCFG.line_size),\n        name=\"Training Accuracy\"\n    ), row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(1, len(val_acc)+1), y=val_acc,\n        mode=\"markers+lines\",\n        marker=dict(\n            color=PlotCFG.valid_color, size=PlotCFG.marker_size,\n            line=dict(color=\"White\", width=0.5)\n        ),\n        line=dict(color=PlotCFG.valid_color, width=PlotCFG.line_size),\n        name=\"Validation Accuracy\"\n    ), row=1, col=2\n)\n\n# Update Axes\nfig.update_xaxes(title=\"Epochs\", linecolor=\"Black\", ticks=\"outside\", row=1, col=1)\nfig.update_xaxes(title=\"Epochs\", linecolor=\"Black\", ticks=\"outside\", row=1, col=2)\nfig.update_yaxes(title=\"Categorical Loss\", linecolor=\"Black\", ticks=\"outside\", row=1, col=1)\nfig.update_yaxes(title=\"Accuracy\", linecolor=\"Black\", ticks=\"outside\", row=1, col=2)\n\n# Update Layout\nfig.update_layout(\n    title=\"Training Loss and Metrics\", title_x=0.5,\n    width=950, height=400,\n    showlegend=False,\n    plot_bgcolor=\"White\",\n    paper_bgcolor=\"White\"\n)\n\n# Show\nfig.show(iframe_connected=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">4.3. Confusion Matrix</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix\nfine_tuned_model = load_model(\"/kaggle/working/asl_vgg16_best_weights.keras\")\npredictions = fine_tuned_model.predict(test_generator)\n\n# Get the true labels from the generator\ntrue_labels = test_generator.classes\n\n# Compute the confusion matrix using tf.math.confusion_matrix\nconfusion_matrix = tf.math.confusion_matrix(\n        labels=true_labels,\n        predictions=predictions.argmax(axis=1),\n        num_classes=29)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Figure\nfig = go.Figure()\n\n# Heatmap\nfig.add_trace(\n    go.Heatmap(\n        z=confusion_matrix,\n        x=labels,\n        y=labels,\n        text=confusion_matrix,\n        texttemplate=\"<b>%{text}</b>\",\n        textfont={\"size\":8},\n        colorscale=[[0, '#f4f4f4'],[1.0, '#76503d']],\n        showscale = False,\n        ygap = 5,\n        xgap = 5,\n        hovertemplate=\n        '''\n        Actual: %{y}<br>\n        Predicted: %{x}<br>\n        Total: %{text}\n        ''',\n        name=\"Confusion Matrix\"\n    )\n)\n\n# Update Axes\nfig.update_xaxes(title=\"<b>Predicted Values</b>\", tickfont_size=10)\nfig.update_yaxes(title=\"<b>Actual Values</b>\", tickfont_size=10)\n\n# Update Layout\nfig.update_layout(title_text='Confusion Matrix', title_x=0.5, font_size=14,\n                  width=1050, \n                  height=1115,\n                  plot_bgcolor='white',\n                  showlegend=False,\n)\n\n# Show\nfig.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style = \"color: Black; \n              display: fill;\n              border-radius: 12px;\n              background-color: #c7a48b;\n              box-shadow: rgba(0, 0, 0, 0.15) 0px 10px 5px, rgba(0, 0, 0, 0.12) 0px 6px 6px;\">\n    <h1 id = \"model-explainability\"\n        style = \"padding: 13px; \n                 color: White;\n                 font-size: 24px;\n                 font-weight: bold;\n                 font-family: Calibri;\">5. | Model Explainability\n        <a class=\"anchor-link\" id=\"model-explainability\" href=\"https://www.kaggle.com/code/harits/vgg16-asl-recognition-model-explainability#model-explainability\">¶</a>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">5.1. t-SNE Visualization</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"dense_model = Model(inputs=fine_tuned_model.inputs, outputs=fine_tuned_model.layers[-3].output)\ndense_model.summary()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(labels)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract Features in Dense Layer\ndef dense_feature_prediction(img_path):\n    img = load_img(img_path, target_size=(CFG.img_height, CFG.img_width))\n    img = img_to_array(img)\n    img = img / 255.\n    img = np.expand_dims(img, axis=0)\n    dense_feature = dense_model.predict(img, verbose=0)[0]\n    return dense_feature\n\nreduction_data = pd.DataFrame()\nfor label in labels:\n    label_data = data_test[data_test[\"label\"]==label][:100]\n    reduction_data = reduction_data.append(label_data)\n\nreduction_data = reduction_data.reset_index(drop=True)\ndisplay(reduction_data)\n\ndense_features = reduction_data[\"image_path\"].progress_apply(dense_feature_prediction)\ndense_features = pd.DataFrame.from_records(dense_features.values, index=dense_features.index)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tSNE Dimensional Reduction\ntsne = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_features = tsne.fit_transform(dense_features)\ntsne_features = pd.DataFrame(tsne_features, columns=[\"tsne_feat_0\", \"tsne_feat_1\"])\nreduction_data[[\"tsne_feat_0\", \"tsne_feat_1\"]] = tsne_features\nreduction_data","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scatter Plot\nlist_colors = [\"#FF0000\", \"#00FF00\", \"#0000FF\", \"#FFFF00\", \"#FF00FF\", \"#00FFFF\", \"#FF4500\", \"#800080\", \"#32CD32\", \"#8B0000\", \n               \"#000080\", \"#808000\", \"#FF6347\", \"#008080\", \"#FF1493\", \"#7FFF00\", \"#D2691E\", \"#9400D3\", \"#B22222\", \"#ADFF2F\",\n               \"#ADD8E6\", \"#FF69B4\", \"#F0E68C\", \"#4682B4\", \"#9ACD32\", \"#800000\", \"#FFD700\", \"#20B2AA\", \"#A52A2A\"\n              ]\nfig = px.scatter(\n    reduction_data, x=\"tsne_feat_0\", y=\"tsne_feat_1\", color='label', color_discrete_sequence=list_colors\n)\n\nfig.update_traces(marker=dict(size=8),)\n\n# Update Axes\nfig.update_xaxes(title=\"\", linecolor=\"Black\", zeroline=False, mirror=True)\nfig.update_yaxes(title=\"\", linecolor=\"Black\", zeroline=False, mirror=True)\n\n# Update Layout\nfig.update_layout(\n    title_text=\"t-SNE Visualization\", title_x=0.5,\n    width=900, height=900,\n    plot_bgcolor='White',\n    coloraxis_showscale=False,\n)\n\n# Show\nfig.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div>\n    <h2 style = \"color: #c7a48b; font-size: 22px; font-family: Calibri; font-weight: bold;\">5.2. Class Activation Maps (Grad-CAM)</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\nclass GradCAM:\n    def __init__(self, model, classIdx, layerName=None):\n        # store the model, the class index used to measure the class\n        # activation map, and the layer to be used when visualizing\n        # the class activation map\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n        \n        # if the layer name is None, attempt to automatically find\n        # the target output layer\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n            \n    def find_target_layer(self):\n        # attempt to find the final convolutional layer in the network\n        # by looping over the layers of the network in reverse order\n        for layer in reversed(self.model.layers):\n            # check to see if the layer has a 4D output\n            if len(layer.output_shape) == 4:\n                return layer.name\n            \n        # otherwise, we could not find a 4D layer so the GradCAM\n        # algorithm cannot be applied\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n        \n    def compute_heatmap(self, image, eps=1e-8):\n        # construct our gradient model by supplying (1) the inputs\n        # to our pre-trained model, (2) the output of the (presumably)\n        # final 4D layer in the network, and (3) the output of the\n        # softmax activations from the model\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer(self.layerName).output, self.model.output])\n        \n        # record operations for automatic differentiation\n        with tf.GradientTape() as tape:\n            # cast the image tensor to a float-32 data type, pass the\n            # image through the gradient model, and grab the loss\n            # associated with the specific class index\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n            \n        # use automatic differentiation to compute the gradients\n        grads = tape.gradient(loss, convOutputs)\n        \n        # compute the guided gradients\n        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n        castGrads = tf.cast(grads > 0, \"float32\")\n        guidedGrads = castConvOutputs * castGrads * grads\n        \n        # the convolution and guided gradients have a batch dimension\n        # (which we don't need) so let's grab the volume itself and\n        # discard the batch\n        convOutputs = convOutputs[0]\n        guidedGrads = guidedGrads[0]\n        \n        # compute the average of the gradient values, and using them\n        # as weights, compute the ponderation of the filters with\n        # respect to the weights\n        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n        \n        # grab the spatial dimensions of the input image and resize\n        # the output class activation map to match the input image\n        # dimensions\n        (w, h) = (image.shape[2], image.shape[1])\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n        \n        # normalize the heatmap such that all values lie in the range\n        # [0, 1], scale the resulting values to the range [0, 255],\n        # and then convert to an unsigned 8-bit integer\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer / denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n        \n        # return the resulting heatmap to the calling function\n        return heatmap\n    \n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n        colormap=cv2.COLORMAP_VIRIDIS):\n        \n        # apply the supplied color map to the heatmap and then\n        # overlay the heatmap on the input image\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        \n        # return a 2-tuple of the color mapped heatmap and the output,\n        # overlaid image\n        return (heatmap, output)","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gradcam_images(labels):\n    # Create Subplots\n    fig, axs = plt.subplots(len(labels), 7, figsize=(12, 10))\n\n    for i, label in enumerate(labels):\n        axs[i, 0].text(0.5, 0.5, label, ha='center', va='center', fontsize=8)\n        axs[i, 0].axis('off')\n        \n        label_data = data_test[data_test[\"label\"]==label][:2].reset_index(drop=True)\n\n        for j in range(2):\n            # Read Original Image\n            orig = cv2.imread(label_data[\"image_path\"][j])\n            orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n            \n            # Preprocess and Predict Label from Image\n            img = load_img(label_data[\"image_path\"][j], target_size=(CFG.img_height, CFG.img_width))\n            img = img_to_array(img) / 255.\n            img = np.expand_dims(img, axis=0)\n            img_label_ci = fine_tuned_model.predict(img, verbose=0)\n            img_label = np.argmax(img_label_ci[0])\n            \n            # Compute Heatmap using GradCAM\n            cam = GradCAM(fine_tuned_model, img_label)\n            heatmap = cam.compute_heatmap(img)\n            \n            # Overlay Heatmap with Original Image\n            heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n            (heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)\n            \n            # Show Original, Heatmap, and Overlap Heatmap Images\n            axs[i, j*3+1].imshow(orig)\n            axs[i, j*3+1].axis(\"off\")\n            axs[i, j*3+2].imshow(heatmap)\n            axs[i, j*3+2].axis(\"off\")\n            axs[i, j*3+3].imshow(output)\n            axs[i, j*3+3].axis(\"off\")\n\n    # Title\n    plt.suptitle(\"Class Activation Maps (GradCAM) in Test Images\", x=0.55, y=0.92)\n\n    # Show\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gradcam_images(labels[:5])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gradcam_images(labels[5:10])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gradcam_images(labels[10:15])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T20:08:22.286Z"}},"outputs":[],"execution_count":null}]}